
# STFDiff: Remote sensing image spatiotemporal fusion with diffusion models

**Abstract:** *Spatiotemporal fusion (STF) methods aim to blend satellite images with different spatial and temporal resolutions to support more frequent and precise monitoring. In the past decades, amounts of STF methods have been developed with remarkable success. However, among the existing methods, the traditional methods rely on the linear assumption and fail for complex and diverse scenes with great dynamics. The deep learning-based methods suffer from the spatial, temporal and spectral uncertainties in STF and the mode collapse problem of generative adversarial networks (GANs) for remote sensing images with complex scenes. To address these problems, we propose a novel spatiotemporal fusion method with diffusion models (STFDiff) that merges a coarse image at the prediction date and the coarse-fine image pairs acquired at other dates to generate the fine image at the prediction date. STFDiff generates the fine image via repeated refinement with initialized Gaussian noise under the control of the prior images acquired at other dates. At each iteration, the noise is predicted through a conditional noise predictor dual-stream Unet (DS-Unet), which enhances the noise features by subtracting the extracted features from the dual-stream encoders (DS-encoders). The noise is then gradually removed, and finally the fine image is generated with similar spatial details to the fine images and temporal dynamics to the coarse images. Comprehensive experiments on two public datasets and one personally collected dataset demonstrate that STFDiff outperforms state-of-the-art (SOTA) methods. To further verify the applicability of STFDiff on downstream tasks, we compared the K-means clustering results on the fusion images generated by different methods. The results show that the classification results of STFDiff are the most consistent with the actual images and obtain $\sim2\%$ mean intersection over union (mIoU) improvement over the SOTA methods. The source code is available at [https://github.com/prowDIY/STF.](https://github.com/prowDIY/STF)*

The overview of STF is as follows:

<div style="text-align: center;">
  <div style="display: flex; justify-content: center; gap: 20px; margin-top: 10px;">
    <figure style="text-align: center;">
      <img title="Flowchart" src="assets/methodology_flowchart.png" height="300" alt="Flowchart">
      <figcaption>(a) Flowchart</figcaption>
    </figure>
    <figure style="text-align: center;">
      <img title="DS-Unet" src="assets/methodology_Unet.png" height="300" alt="DS-Unet">
      <figcaption>(b) DS-Unet</figcaption>
    </figure>
  </div>
  <figcaption><strong>The overview of STFDiff</strong></figcaption>
</div>

Our paper STFDiff has been accepted by Information Fusion.[[link]](https://www.sciencedirect.com/science/article/pii/S1566253524002835)

This repository offers a unified codebase for spatiotemporal fusion, including STFDCNN, STFNet, SwinSTF, GANSTFM, STFGAN, OPGAN, STFDiff and so on.
This repository is currently under organization. Training weights for various models and additional spatiotemporal fusion (STF) methods will be released gradually.

Additionally, we have reimplemented several traditional methods in Python, utilizing GPU acceleration. This part of the code is currently undergoing comparison testing with the original implementations, and we cannot guarantee that the results will match those of the original code at this stage.

## Usage

### Prepare the Environment

```bash
git clone https://github.com/huang-he99/STF.git
pip install -e .
```

### Training

```bash
python tools/train/train_**.py --congfig_path config/**.py
```

### Inference

```bash
python tools/inference/test_**.py --congfig_path config/**.py
```

### Dataset

#### Download Link

| Dataset       | CIA                                             | LGC                                             | AHB/Tianjin/Daxing                                                                                                                                  | E-Smile                                                           |
| ------------- | ----------------------------------------------- | ----------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------- |
| Download Link | [Link](http://dx.doi.org/10.4225/08/5111AC0BF1229) | [Link](http://dx.doi.org/10.4225/08/5111AC0BF1229) | [Google Drive](https://drive.google.com/drive/folders/1yzw-4TaY6GcLPIRNFBpchETrFKno30he) / [Baidu Cloud](https://pan.baidu.com/s/1ymgud6tnY6XB5CTCXPUfnw) | [Link](https://www.kaggle.com/datasets/yuxiawhu/extra-data-in-smile) |

#### Data Process

For the CIA/LGC dataset, the raw files are in BIL or BSQ format. These data types can be converted to .tif format using the following Python code:

```python
import numpy as np
# BIL: rows band columns
with open('L71093084_08420011007_HRF_modtran_surf_ref_agd66.bil', 'rb') as f:
    data = f.read()
    data = np.frombuffer(data, dtype='uint16')
    data = data.reshape(2040 ,6, 1720).transpose(0,2,1)
    tiff.imsave('L.tif', data)

# BSQ: band rows columns
with open('MOD09GA_A2001281.sur_refl.int', 'rb') as f:
    data = f.read()
    data = np.frombuffer(data, dtype='uint16')
    data = data.reshape(6,2040 , 1720).transpose(1,2,0)
    tiff.imsave('M.tif', data)
```

For convenience, we rename each file as the format [sensor_type]_[year-month-day].tif and remove the invalid pixels.

```bash
python -m scripts.spatio_temparol_fusion.format.format --root_path data/spatio_temporal_fusion --src_data_prefix raw_data --tar_data_prefix format_data
python -m  scripts.spatio_temparol_fusion.process.crop --root_path data/spatio_temporal_fusion --src_data_prefix public_processing_data/format_data --tar_data_prefix public_processing_data/format_data/crop_{}_{}_{}_{}
```

Then we process the data according to a predefined procedure, such as selecting the specific bands and splitting the data into sub-patch.
For example, we can run the following codes to generate the data whose spatial size is 256 and spectral band is 3 (NIR-Red-Blue).

```bash
python -m scripts.spatio_temparol_fusion.process.select_bands --root_path data/spatio_temporal_fusion --src_data_prefix public_processing_data/format_data/crop_{}_{}_{}_{} --tar_data_prefix public_processing_data/format_data/crop_{}_{}_{}_{}/band_{} --band_list 4 3 2
python -m scripts.spatio_temparol_fusion.process.split --root_path data/spatio_temporal_fusion --src_data_prefix public_processing_data/format_data/crop_{}_{}_{}_{}/band_4-3-2 --tar_data_prefix public_processing_data/format_data/crop_{}_{}_{}_{}/band_4-3-2/split_size_{}_stride_{} --img_patch_size 256 --stride 128
```

Finally we can generate the corresponding dataset according to the options.

```bash
python -m scripts.spatio_temparol_fusion.dataset_generation.data_generation --root_path data/spatio_temporal_fusion --src_data_prefix public_processing_data/format_data/split_size_256_stride_128 --tar_data_prefix hh_setting-1-patch --dataset_setting_congfig_path scripts/spatio_temparol_fusion/dataset_generation/dataset_config/hh_setting.py
```

## Citation

If this repo helps you, please consider citing our works:

```tex
@article{huang2024stfdiff,
  title={STFDiff: Remote sensing image spatiotemporal fusion with diffusion models},
  author={Huang, He and He, Wei and Zhang, Hongyan and Xia, Yu and Zhang, Liangpei},
  journal={Information Fusion},
  pages={102505},
  year={2024},
  publisher={Elsevier}
}

@article{song2022remote,
  title={Remote sensing image spatiotemporal fusion via a generative adversarial network with one prior image pair},
  author={Song, Yiyao and Zhang, Hongyan and Huang, He and Zhang, Liangpei},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={60},
  pages={1--17},
  year={2022},
  publisher={IEEE}
}
```
